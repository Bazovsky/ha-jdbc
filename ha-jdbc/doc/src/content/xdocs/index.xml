<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE document PUBLIC "-//APACHE//DTD Documentation V1.2//EN" "http://xml.apache.org/forrest/dtd/document-v12.dtd">
<document> 
	<header> 
		<title>HA-JDBC: High-Availability JDBC</title>
	</header> 
	<body>
		<section>
			<title>Introduction</title>
			<section>
				<title>What is HA-JDBC?</title>
				<p>
					HA-JDBC is a JDBC driver implementation that provides light-weight, transparent clustering capability to groups of homogeneous JDBC-accessed databases.
				</p>
			</section>
			<section>
				<title>Why is it useful?</title>
				<p>
					This project began as an effort to provide a "better-than-nothing" solution to the "single point of failure" problem that plagues many low-budget database-driven software projects.
					Simply put, the current solutions for database redundancy are less than ideal:
				</p>
				<ul>
					<li>Commercial clustering solutions are <em>very</em> expensive.</li>
					<li>Several open source replication solutions exist (e.g. <link href="http://www.mysql.com/products/mysql/">MySQL</link> replication, <link href="http://gborg.postgresql.org/project/erserver">PostgreSQL eRServer</link>), but are inadequate:</li>
					<ul>
						<li>Asynchronous replication solutions perform adequately but impose a replication latency and do not provide transactional concurrency.</li>
						<li>Synchronous replication solutions may provide transactional concurrency but often impose a high performance cost.</li>
						<li>The asymmetric master-slave design does not provide fail-over of the master database.</li>
					</ul>
					<li>
						The <link href="http://c-jdbc.objectweb.org">C-JDBC</link> project, hosted at <link href="http://www.objectweb.org">ObjectWeb</link>, makes an impressive attempt at providing a full database clustering solution.
						C-JDBC provides many sophisticated features, but due to the nature of its design, limits the functionality of the underlying JDBC drivers and is unable to support many JDBC 3.0 features.
						These issues aside, it does many things well and is definitely worth checking out.
					</li>
				</ul>
				<p>
				</p>
			</section>
			<section>
				<title>What features does HA-JDBC provide?</title>
				<ul>
					<li>Support for full JDBC 3.0 (Java 1.4) feature set.</li>
					<li>Transparent integration.</li>
					<li>Allows use of vendor implementations of SQL types (e.g. <code>Blob</code>, <code>Clob</code>, <code>Ref</code>, etc).</li>
					<li>Improves performance of concurrent read-access by distributing load across individual cluster nodes.</li>
					<li>Ensures data concurrency by deactivating failed cluster nodes.</li>
					<li>Library of cluster-aware <link href="http://www.jcp.org/en/jsr/detail?id=114">JSR 114</link> <code>javax.sql.RowSet</code> implementations (coming soon)</li>
					<li>Configurable node failure strategies (coming soon)</li>
					<li>Configurable distributed cluster manager strategies (coming soon)</li>
					<li>Cluster health introspection (coming soon)</li>
				</ul>
			</section>
			<section>
				<title>What features does HA-JDBC <strong>NOT</strong> provide?</title>
				<ul>
					<li>Distributed queries.</li>
					<li>Data partitioning</li>
					<li>Hot activation/synchronization of new/failed cluster nodes.</li>
					<li><code>ResultSet</code> caching.</li>
					<li>Heterogeneous clusters (i.e. different database vendors within in the same cluster).</li>
					<li>Support for write operations (i.e. <code>setXXX(...)</code>) in third-party <code>javax.sql.RowSet</code> implementations.</li>
				</ul>
			</section>
		</section>
		<section>
			<title>Configuration</title>
			<section>
				<title>Cluster Configuration</title>
				<p>
					Configuration for HA-JDBC managed database clusters is contained in a single XML file.
					The format of the configuration file should match the DTD provided in the distribution: <code>conf/dtd/ha-jdbc.dtd</code>
				</p>
				<p>Sample configuration file:</p>
				<p>
					<code><![CDATA[<?xml version="1.0"?>]]></code><br/>
					<code><![CDATA[<!DOCTYPE ha-jdbc SYSTEM "ha-jdbc.dtd">]]></code><br/>
					<code><![CDATA[<ha-jdbc>]]></code><br/>
					<code>&nbsp;&nbsp;<![CDATA[<local><!-- These clusters will only be referenced from one JVM -->]]></code><br/>
					<code>&nbsp;&nbsp;&nbsp;&nbsp;<![CDATA[<cluster name="jdbc:ha-jdbc:cluster" validate-sql="SELECT 1">]]></code><br/>
					<code>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<![CDATA[<database url="jdbc:postgresql://server1:5432/database" driver="org.postgresql.Driver">]]></code><br/>
					<code>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<![CDATA[<user>postgres</user>]]></code><br/>
					<code>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<![CDATA[<password>password</password>]]></code><br/>
					<code>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<![CDATA[</database>]]></code><br/>
					<code>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<![CDATA[<database url="jdbc:postgresql://server2:5432/database" driver="org.postgresql.Driver">]]></code><br/>
					<code>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<![CDATA[<user>postgres</user>]]></code><br/>
					<code>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<![CDATA[<password>password</password>]]></code><br/>
					<code>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<![CDATA[</database>]]></code><br/>
					<code>&nbsp;&nbsp;&nbsp;&nbsp;<![CDATA[</cluster>]]></code><br/>
					<code>&nbsp;&nbsp;&nbsp;&nbsp;<![CDATA[<cluster name="java:comp/env/jdbc/Cluster" validate-sql="SELECT 1">]]></code><br/>
					<code>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<![CDATA[<datasource name="java:comp/env/jdbc/Database1">]]></code><br/>
					<code>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<![CDATA[<user>postgres</user>]]></code><br/>
					<code>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<![CDATA[<password>password</password>]]></code><br/>
					<code>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<![CDATA[</datasource>]]></code><br/>
					<code>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<![CDATA[<datasource name="java:comp/env/jdbc/Database2">]]></code><br/>
					<code>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<![CDATA[<user>postgres</user>]]></code><br/>
					<code>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<![CDATA[<password>password</password>]]></code><br/>
					<code>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<![CDATA[</datasource>]]></code><br/>
					<code>&nbsp;&nbsp;&nbsp;&nbsp;<![CDATA[</cluster>]]></code><br/>
					<code>&nbsp;&nbsp;<![CDATA[</local>]]></code><br/>
					<code><![CDATA[</ha-jdbc>]]></code><br/>
				</p>
				<p>
					The algorithm for locating the configuration file resource is as follows:<br/>
				</p>
				<ol>
					<li>Read the resource name from the <code>ha-jdbc.configuration</code> system property.  If not defined, the default value "<code>ha-jdbc.xml</code>" is used.</li>
					<li>Attempt to interpret the resource name as a URL.</li>
					<li>If the resource name cannot be converted to a URL, then search for the resource in the classpath using the following algorithm:</li>
					<ol>
						<li>Search for resource using the thread context class loader.</li>
						<li>If not found, search for resource using the class loader of the current class.</li>
						<li>If not found, search for resource using the system class loader.</li>
					</ol>
					<li>If still not found, throw an exception back to the caller.</li>
				</ol>
			</section>
			<section>
				<title>Application Configuration</title>
				<section>
					<title>DriverManager-based access</title>
					<ol>
						<li>As with your existing JDBC drivers, the HA-JDBC driver must first be loaded.  This can be accomplished in one of two ways:<br/>
							<br/>
							<ul>
								<li>Add <code>net.sf.ha.jdbc.DriverProxy</code> to your <code>jdbc.drivers</code> system property.</li>
								<li>Explicity load the <code>net.sf.ha.jdbc.DriverProxy</code> class using <code>Class.forName(...)</code>.<br/>
									<p>
										Per the JDBC specification, loading the HA-JDBC driver class automatically registers the driver with the <code>DriverManager</code>.
										The HA-JDBC driver will automatically load all underlying JDBC drivers defined within a given cluster.
									</p>
								</li>
							</ul>
							<br/>
						</li>
						<li>The URL to use in subsequent calls to <code>DriverManager.connect(...)</code> should be the name of your cluster.  So that your URLs do not collide with those of other drivers, the following convention is recommended:<br/>
							<p><code>jdbc:ha-jdbc:</code><em>cluster-name</em></p>
							<p>e.g.</p>
							<p>
								<code>Connection connection = DriverManager.connect("jdbc:ha-jdbc:cluster", "postgres", "password");</code>
							</p>
						</li>
					</ol>
				</section>
				<section>
					<title>DataSource-based access</title>
					<ol>
						<li>If your server is configured to use a DataSource implementation provided by your JDBC driver, then add an HA-JDBC DataSource using the appropriate class name to your server configuration along side your existing JDBC DataSources:<br/>
							<br/>
							<ul>
								<li><code>net.sf.ha.jdbc.DataSourceProxy</code></li>
								<li><code>net.sf.ha.jdbc.pool.ConnectionPoolDataSourceProxy</code></li>
								<li><code>net.sf.ha.jdbc.pool.xa.XADataSourceProxy</code></li>
							</ul>
							<br/>
						</li>
						<li>If your server is configured to use a DataSource implementation <em>NOT</em> provided by your JDBC driver, then replace the Driver class name, URL, and username/password values in the existing DataSource definition with the HA-JDBC values as described in the "DriverManager-based access" section.<br/><br/></li>
						<li>Use the JNDI name configured in your cluster configuration file to access the HA-JDBC DataSource.<br/>
							<p>e.g.</p>
							<p>
								<code>Context context = new InitialContext();</code><br/>
								<code>DataSource dataSource = (DataSource) context.lookup("java:comp/env/jdbc/Cluster");</code><br/>
								<code>Connection connection = dataSource.getConnection("postgres", "password");</code><br/>
							</p>
						</li>
					</ol>
				</section>
				<section>
					<title>Server configuration examples</title>
					<ul>
						<li><p><link href="http://jetty.mortbay.org">Jetty</link> (using <link href="http://jotm.objectweb.org">JOTM</link> and <link href="http://xapool.objectweb.org">XAPool</link>)</p>
							<p>
								<code><![CDATA[<Configure class="org.mortbay.jetty.plus.Server">]]></code><br/>
								<code>&nbsp;&nbsp;<![CDATA[<Call name="addService">]]></code><br/>
								<code>&nbsp;&nbsp;&nbsp;&nbsp;<![CDATA[<Arg>]]></code><br/>
								<code>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<![CDATA[<New class="org.mortbay.jetty.plus.JotmService">]]></code><br/>
								<code>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<![CDATA[<Set name="Name">TransactionManager</Set>]]></code><br/>
								<code>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<![CDATA[<Call name="addDataSource">]]></code><br/>
								<code>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<![CDATA[<Arg>jdbc/database</Arg>]]></code><br/>
								<code>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<![CDATA[<Arg>]]></code><br/>
								<code>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<![CDATA[<New class="org.enhydra.jdbc.standard.StandardXADataSource">]]></code><br/>
								<code>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<![CDATA[<Set name="DriverName">net.sf.ha.jdbc.DriverProxy</Set>]]></code><br/>
								<code>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<![CDATA[<Set name="Url">jdbc:ha-jdbc:cluster1</Set>]]></code><br/>
								<code>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<![CDATA[<Set name="User">postgres</Set>]]></code><br/>
								<code>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<![CDATA[<Set name="Password">password</Set>]]></code><br/>
								<code>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<![CDATA[<Set name="TransactionIsolation">]]></code><br/>
								<code>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<![CDATA[<Get class="java.sql.Connection" name="TRANSACTION_READ_COMMITTED"/>]]></code><br/>
								<code>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<![CDATA[</Set>]]></code><br/>
								<code>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<![CDATA[</New>]]></code><br/>
								<code>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<![CDATA[</Arg>]]></code><br/>
								<code>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<![CDATA[<Arg>]]></code><br/>
								<code>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<![CDATA[<New class="org.enhydra.jdbc.pool.StandardXAPoolDataSource">]]></code><br/>
								<code>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<![CDATA[<Arg type="Integer">5</Arg>]]></code><br/>
								<code>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<![CDATA[<Set name="MinSize">5</Set>]]></code><br/>
								<code>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<![CDATA[<Set name="MaxSize">50</Set>]]></code><br/>
								<code>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<![CDATA[</New>]]></code><br/>
								<code>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<![CDATA[</Arg>]]></code><br/>
								<code>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<![CDATA[</Call>]]></code><br/>
								<code>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<![CDATA[</New>]]></code><br/>
								<code>&nbsp;&nbsp;&nbsp;&nbsp;<![CDATA[</Arg>]]></code><br/>
								<code>&nbsp;&nbsp;<![CDATA[</Call>]]></code><br/>
								<code><![CDATA[</Configure>]]></code><br/>
							</p>
						</li>
						<li><p><link href="http://jakarta.apache.org/tomcat">Tomcat</link> (using <link href="http://jakarta.apache.org/commons/dbcp">DBCP</link>)</p>
							<p>
								<code><![CDATA[<Resource name="jdbc/database" auth="Container" type="javax.sql.DataSource"/>]]></code><br/>
								<code><![CDATA[<ResourceParams name="jdbc/database">]]></code><br/>
								<code>&nbsp;&nbsp;<![CDATA[<parameter>]]></code><br/>
								<code>&nbsp;&nbsp;&nbsp;&nbsp;<![CDATA[<name>factory</name>]]></code><br/>
								<code>&nbsp;&nbsp;&nbsp;&nbsp;<![CDATA[<value>org.apache.commons.dbcp.BasicDataSourceFactory</value>]]></code><br/>
								<code>&nbsp;&nbsp;<![CDATA[</parameter>]]></code><br/>
								<code>&nbsp;&nbsp;<![CDATA[<parameter>]]></code><br/>
								<code>&nbsp;&nbsp;&nbsp;&nbsp;<![CDATA[<name>maxActive</name>]]></code><br/>
								<code>&nbsp;&nbsp;&nbsp;&nbsp;<![CDATA[<value>100</value>]]></code><br/>
								<code>&nbsp;&nbsp;<![CDATA[</parameter>]]></code><br/>
								<code>&nbsp;&nbsp;<![CDATA[<parameter>]]></code><br/>
								<code>&nbsp;&nbsp;&nbsp;&nbsp;<![CDATA[<name>maxIdle</name>]]></code><br/>
								<code>&nbsp;&nbsp;&nbsp;&nbsp;<![CDATA[<value>30</value>]]></code><br/>
								<code>&nbsp;&nbsp;<![CDATA[</parameter>]]></code><br/>
								<code>&nbsp;&nbsp;<![CDATA[<parameter>]]></code><br/>
								<code>&nbsp;&nbsp;&nbsp;&nbsp;<![CDATA[<name>maxWait</name>]]></code><br/>
								<code>&nbsp;&nbsp;&nbsp;&nbsp;<![CDATA[<value>10000</value>]]></code><br/>
								<code>&nbsp;&nbsp;<![CDATA[</parameter>]]></code><br/>
								<code>&nbsp;&nbsp;<![CDATA[<parameter>]]></code><br/>
								<code>&nbsp;&nbsp;&nbsp;&nbsp;<![CDATA[<name>username</name>]]></code><br/>
								<code>&nbsp;&nbsp;&nbsp;&nbsp;<![CDATA[<value>postgres</value>]]></code><br/>
								<code>&nbsp;&nbsp;<![CDATA[</parameter>]]></code><br/>
								<code>&nbsp;&nbsp;<![CDATA[<parameter>]]></code><br/>
								<code>&nbsp;&nbsp;&nbsp;&nbsp;<![CDATA[<name>password</name>]]></code><br/>
								<code>&nbsp;&nbsp;&nbsp;&nbsp;<![CDATA[<value>password</value>]]></code><br/>
								<code>&nbsp;&nbsp;<![CDATA[</parameter>]]></code><br/>
								<code>&nbsp;&nbsp;<![CDATA[<parameter>]]></code><br/>
								<code>&nbsp;&nbsp;&nbsp;&nbsp;<![CDATA[<name>driverClassName</name>]]></code><br/>
								<code>&nbsp;&nbsp;&nbsp;&nbsp;<![CDATA[<value>net.sf.ha.jdbc.DriverProxy</value>]]></code><br/>
								<code>&nbsp;&nbsp;<![CDATA[</parameter>]]></code><br/>
								<code>&nbsp;&nbsp;<![CDATA[<parameter>]]></code><br/>
								<code>&nbsp;&nbsp;&nbsp;&nbsp;<![CDATA[<name>url</name>]]></code><br/>
								<code>&nbsp;&nbsp;&nbsp;&nbsp;<![CDATA[<value>jdbc:ha-jdbc:cluster1</value>]]></code><br/>
								<code>&nbsp;&nbsp;<![CDATA[</parameter>]]></code><br/>
								<code><![CDATA[</ResourceParams>]]></code><br/>
							</p>
						</li>
						<li><p>More server configuration examples coming soon...</p></li>
					</ul>
				</section>
			</section>
			<section>
				<title>Distributed environment</title>
				<p>
					When running HA-JDBC in a distributed environment, i.e. JDBC clients in seperate JVMs are accessing the same set of databases, special configuration is required.
					HA-JDBC leverages the <link href="http://www.jgroups.org">JGroups</link> project to handle communication between cluster managers.
					See the JGroup User's guide for assistance customizing the protocol stack for your network.
				</p>
				<p>e.g.</p>
				<p>
					<code><![CDATA[<?xml version="1.0"?>]]></code><br/>
					<code><![CDATA[<!DOCTYPE ha-jdbc SYSTEM "ha-jdbc.dtd">]]></code><br/>
					<code><![CDATA[<ha-jdbc>]]></code><br/>
					<code>&nbsp;&nbsp;<![CDATA[<distributable protocol="UDP(mcast_addr=228.1.2.3;mcast_port=45566;ip_ttl=0;trace=true):]]></code><br/>
					<code>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<![CDATA[PING(timeout=3000;num_initial_members=6):]]></code><br/>
					<code>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<![CDATA[FD(trace=true;timeout=5000):]]></code><br/>
					<code>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<![CDATA[VERIFY_SUSPECT(trace=false;timeout=1500):]]></code><br/>
					<code>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<![CDATA[DISCARD(trace=true;down=0.2):]]></code><br/>
					<code>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<![CDATA[pbcast.STABLE(trace=true;desired_avg_gossip=5000):]]></code><br/>
					<code>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<![CDATA[pbcast.NAKACK(trace=true;gc_lag=5;retransmit_timeout=3000):]]></code><br/>
					<code>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<![CDATA[UNICAST(timeout=5000):]]></code><br/>
					<code>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<![CDATA[FRAG:]]></code><br/>
					<code>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<![CDATA[pbcast.GMS(join_timeout=5000;join_retry_timeout=2000;trace=true;shun=false;print_local_addr=false)">]]></code><br/>
					<code>&nbsp;&nbsp;&nbsp;&nbsp;<![CDATA[<cluster name="...">]]></code><br/>
					<code>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<![CDATA[<!-- ... -->]]></code><br/>
					<code>&nbsp;&nbsp;&nbsp;&nbsp;<![CDATA[</cluster>]]></code><br/>
					<code>&nbsp;&nbsp;<![CDATA[</distributable>]]></code><br/>
					<code><![CDATA[</ha-jdbc>]]></code><br/>
				</p>
			</section>
			<section>
				<title>Unique identifier generation</title>
				<p>
					The following traditional mechanisms for generating unique identifiers are problematic for clustered database environments:
				</p>
				<ul>
					<li>Identity columns (supported by MySQL, HSQLDB, DB2, MS SQL Server, Sybase, etc.) fetched using the Java 1.4 <code>ResultSet.getGeneratedKeys()</code> method</li>
					<li>Sequences (supported by PostgreSQL, MySQL MaxDB (fka SAP DB), Firebird (fka Interbase), Mckoi, Oracle, DB2, etc.)</li>
					<li>Incrementing columns (e.g. <code>SELECT max(id) + 1 FROM table</code>)</li>
				</ul>
				<p>
					For any of these mechanisms to work in a database cluster, each database node would need to generate uniform identifier independently.
					To do this, all incoming database requests would need to be synchronized through a central cluster manager.
					This would adversely affect the scalability of the cluster as the number of distributed clients increase.
					For now, HA-JDBC does not accommodate the above mechanisms in favor a design that uses more efficient asynchronous request execution.
				</p>
				<p>
					The following alternative unique identifier generation methods will work properly with HA-JDBC:
				</p>
				<ul>
					<li>High-Low algorithm (often implemented using a Stateless Session EJB)</li>
					<li>UUID algorithms</li>
					<li>Others?</li>
				</ul>
			</section>
			<section>
				<title>Transactions</title>
				<p>
					HA-JDBC supports both traditional JDBC transactions and JTA.
					HA-JDBC integration will not require you to change your existing transaction semantics.
					If you are leveraging your JDBC driver's <code>javax.sql.XADataSource</code> implementation for JTA integration, you will need to configure HA-JDBC accordingly.
				</p>
				<p>e.g.</p>
				<p>
					<code>&nbsp;&nbsp;<![CDATA[<cluster name="java:comp/env/jdbc/Cluster">]]></code><br/>
					<code>&nbsp;&nbsp;&nbsp;&nbsp;<![CDATA[<xa-datasource name="java:comp/env/jdbc/xa/DataSource1">]]></code><br/>
					<code>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<![CDATA[<user>postgres</user>]]></code><br/>
					<code>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<![CDATA[<password>password</password>]]></code><br/>
					<code>&nbsp;&nbsp;&nbsp;&nbsp;<![CDATA[</xa-datasource>]]></code><br/>
					<code>&nbsp;&nbsp;&nbsp;&nbsp;<![CDATA[<xa-datasource name="java:comp/env/jdbc/xa/DataSource2">]]></code><br/>
					<code>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<![CDATA[<user>postgres</user>]]></code><br/>
					<code>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<![CDATA[<password>password</password>]]></code><br/>
					<code>&nbsp;&nbsp;&nbsp;&nbsp;<![CDATA[</xa-datasource>]]></code><br/>
					<code>&nbsp;&nbsp;<![CDATA[</cluster>]]></code><br/>
				</p>
			</section>
			<section>
				<title>Connection Pooling</title>
				<p>
					HA-JDBC is designed to integrate with your existing connection pooling mechanism.
					If you are leveraging your JDBC driver's <code>javax.sql.ConnectionPoolDataSource</code> implementation for connection pooling, you will need to configure HA-JDBC accordingly.
				</p>
				<p>e.g.</p>
				<p>
					<code>&nbsp;&nbsp;<![CDATA[<cluster name="java:comp/env/jdbc/Cluster">]]></code><br/>
					<code>&nbsp;&nbsp;&nbsp;&nbsp;<![CDATA[<pool-datasource name="java:comp/env/jdbc/pool/DataSource1">]]></code><br/>
					<code>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<![CDATA[<user>postgres</user>]]></code><br/>
					<code>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<![CDATA[<password>password</password>]]></code><br/>
					<code>&nbsp;&nbsp;&nbsp;&nbsp;<![CDATA[</pool-datasource>]]></code><br/>
					<code>&nbsp;&nbsp;&nbsp;&nbsp;<![CDATA[<pool-datasource name="java:comp/env/jdbc/pool/DataSource2">]]></code><br/>
					<code>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<![CDATA[<user>postgres</user>]]></code><br/>
					<code>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<![CDATA[<password>password</password>]]></code><br/>
					<code>&nbsp;&nbsp;&nbsp;&nbsp;<![CDATA[</pool-datasource>]]></code><br/>
					<code>&nbsp;&nbsp;<![CDATA[</cluster>]]></code><br/>
				</p>
			</section>
			<section>
				<title>Advanced JDBC Features</title>
				<p>
					Coming soon...
				</p>
			</section>
			<section>
				<title>Integration with O/R Mapping Frameworks</title>
				<p>
					Coming soon...
				</p>
			</section>
		</section>
		<section>
			<title>Architecture</title>
			<p>
				JDBC operations are divided into 4 categories:
			</p>
			<dl>
				<dt>Read operations (e.g. SELECT statements)</dt>
				<dd>Deep read operations on the underlying database driver that will require read access to the database itself.</dd>
				<dt>Write operations (e.g. INSERT, UPDATE, DELETE statements)</dt>
				<dd>Deep write operations on the underlying database driver that will require write access to the database itself.</dd>
				<dt>Get operations (e.g. getXXX())</dt>
				<dd>Shallow read operations on the underlying database driver that will NOT require any access to the database itself.</dd>
				<dt>Set operations (e.g. setXXX())</dt>
				<dd>Shallow write operations on the underlying database driver that will NOT require any access to the database itself.</dd>
			</dl>
			<p>
				Operation are executed differently according to the type operation:
			</p>
			<dl>
				<dt>Read Operations</dt>
				<dd>Delegated to a single node in the cluster in round-robin fashion.</dd>
				<dt>Write Operations</dt>
				<dd>Delegated in parallel to each node in the cluster.</dd>
				<dt>Get Operations</dt>
				<dd>Delegated to the first node in the cluster.</dd>
				<dt>Set Operations</dt>
				<dd>Serially delegated to each node in the cluster.</dd>
			</dl>
			<p>
				Exceptions caught while performing an operation are handled differently according to the type of operation:
			</p>
			<dl>
				<dt>Read Operations</dt>
				<dd>Perform validation check on failed database.  If successful, return the caught exception to the caller.  If unsuccessful, the database is deactivated and the request is forwarded to the next node in the cluster.</dd>
				<dt>Write Operations</dt>
				<dd>Perform validation check on failed database.  If successful, return the caught exception to the caller.  If unsuccessful, the database is deactivated and the results of the succeeding database nodes are returned to the caller.</dd>
				<dt>Get Operations</dt>
				<dd>The exception is immediately returned to the caller.</dd>
				<dt>Set Operations</dt>
				<dd>The exception is immediately returned to the caller.</dd>
			</dl>
		</section>
	</body>
</document>
